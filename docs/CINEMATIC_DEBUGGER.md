# ğŸ¬ Cinematic Debugger for Robot Psychology

A directorâ€™s commentary track for Loopforge City from The Producer.

This document explains how to watch the simulation like a film, not a stack trace.

It covers:

- What the â€œcinematic debuggerâ€ is
- How telemetry flows into narrative layers
- What each CLI view does (`--narrative`, `--recap`, `--daily-log`, `explain-episode`, `lens-agent`)
- How to interpret those outputs as â€œrobot psychologyâ€
- Where real LLMs will eventually plug in (and what contracts they must honor)

The goal:
You should be able to run a multi-day sim and say things like:

â€œDelta is burning out under guardrails while the factory cools down, and Nova is chilling in QA heaven.â€

â€¦without reading a single raw log line.

## 0. Mental Model

Think of the system in layers:

### 1. Simulation
`Environment â†’ AgentPerception â†’ Policy â†’ AgentActionPlan â†’ legacy dict â†’ Environment`

### 2. Telemetry (logging)

* `ActionLogEntry` JSONL (one per agent decision)
* No story yet, just facts: step, agent, mode, location, stress, outcome, etc.

### 3. Summaries (stats)

* `DaySummary` / `AgentDayStats`
* `EpisodeSummary` / `AgentEpisodeStats`
* Derived from telemetry only (no reflection / LLM magic).

### 4. Views (cinematic debugger) â€“ all read-only over summaries:

* `narrative_viewer.py` â†’ per-day story beats (`--narrative`)
* `episode_recaps.py` â†’ high-level recap (`--recap`)
* `daily_logs.py` â†’ â€œops log meets diaryâ€ (`--daily-log`)
* `explainer.py` â†’ dev-facing agent explainer (`explain-episode`)
* `llm_lens.py` â†’ typed contexts for future LLMs (`lens-agent`)

Core rule:
**Simulation doesnâ€™t know this exists.**
We only watch; we donâ€™t push back into behavior yet.

## 1. Data Contracts

These are the spine of the cinematic debugger. Donâ€™t break them casually.

### 1.1 Telemetry â†’ Summaries

* `ActionLogEntry` (in logs)
  * `step`, `agent_name`, `mode` (`guardrail` or `context`), `location`
  * `perception.emotions.stress` (float)
  * `outcome` (`"ok"`, `"incident"`, etc.)
* `DaySummary`
  * `day_index`
  * `tension` (0..1) â€” heuristic from stress + incidents
  * `perception_mode` (e.g. `"accurate"` for now)
  * `agent_stats: Dict[name, AgentDayStats]`
* `AgentDayStats`
  * `name`, `role`
  * `guardrail_count`, `context_count`
  * `avg_stress`
  * `incidents`
  * `reflection` (single representative string, for flavor only)
* `EpisodeSummary`
  * `tension_values: List[float]` (per day)
  * `agent_stats: Dict[name, AgentEpisodeStats]`
* `AgentEpisodeStats`
  * `name`, `role`
  * `total_guardrail`, `total_context`
  * `stress_start`, `stress_end`
  * `visual`, `vibe`, `tagline` from `CHARACTERS` registry

**Important invariant:**
All **numbers** in the cinematic debugger are derived from telemetry (ActionLogEntry â†’ DaySummary â†’ EpisodeSummary), not from reflections or LLM-like text.

## 2. Characters & Style
### 2.1 Character Registry

`loopforge/characters.py` defines:

```
CHARACTERS = {
  "Delta": {
    "role": "optimizer",
    "visual": "tall, angular frame, factory overalls",
    "vibe": "anxious efficiency nerd",
    "tagline": "Always chasing the perfect throughput."
  },
  "Nova": {
    "role": "qa",
    "visual": "sleek, sensor-heavy shell",
    "vibe": "calm forensic inspector",
    "tagline": "Nothing leaves without a second look."
  },
  "Sprocket": {
    "role": "maintenance",
    "visual": "oil-stained chassis with a toolkit belt",
    "vibe": "quiet fixer",
    "tagline": "Keeps the bones of the city alive."
  },
  ...
}
```

This doesnâ€™t affect behavior. It colors the reporting:
* Character sheets
* Day narratives intros
* Daily logs intros
* Explainer flavor lines

If you add agents later, give them an entry here so they show up with personality.

## 3. Viewer Layers
### 3.1 Day Narratives (`narrative_viewer.py` â†’ `--narrative`)

API:
```
build_day_narrative(day_summary, day_index, previous_day_summary=None)
```

Produces a `DayNarrative`:
* `intro: str` â€“ â€œThe floor is steady with a subtle edge.â€
* `agent_beats: List[AgentDayBeat]` â€“ 3â€“5 lines per agent
* `supervisor_line: str`
* `outro: str` â€“ tension-trend aware

**Example output (real-ish):**

```
Day 1 â€” The factory feels focused but calm.
  [Delta (optimizer)]
    Delta comes online steady but alert â€” always chasing efficiency.
    Delta seems unbothered and leans heavily on the rulebook.
    Mostly pushes the line for output, by the manual.
    Ends the day balanced, tension kept in check.
  [Nova (qa)]
    Nova drifts into the shift almost relaxed â€” ever watchful for cracks in the system.
    ...
  [Sprocket (maintenance)]
    ...
  Supervisor: Supervisor keeps a steady watch but rarely intervenes.
  The shift winds down lighter than it began; the floor exhales a little.
```

**How to interpret:**
* Day intro â†’ one-liner about tension:
  * high â†’ â€œsharpâ€, â€œedgyâ€
  * mid â†’ â€œsteady with a subtle edgeâ€
  * low â†’ â€œhums quietly; nothing feels urgentâ€
* Agent intro â†’ role flavor + stress band:
  * high stress â†’ â€œwound a little tightâ€
  * mid â†’ â€œsteady but alertâ€
  * low â†’ â€œrelaxed and lightâ€
* â€œLeans heavily on the rulebookâ€
â†’ guardrail-only behavior (all steps in guardrail mode).
Later, when context usage appears, weâ€™ll get lines about â€œacting on instinct / improvisingâ€.

Use this view for: **Quick story of each day** for humans.

### 3.2 Episode Recap (`episode_recaps.py` â†’ `--recap`)

API:
```
build_episode_recap(episode_summary, day_summaries, characters)
```

**Example:**

```
EPISODE RECAP
==============================
The episode eases off; the early edge softens over time.
- Delta: Delta (optimizer) moved from high stress to low and gradually unwound. stayed strictly within guardrails.
- Nova: Nova (qa) moved from low stress to low and gradually unwound. stayed strictly within guardrails.
- Sprocket: Sprocket (maintenance) moved from mid stress to low and gradually unwound. stayed strictly within guardrails.
The shift winds down quietly, nothing pressing.
```

**How to interpret:**
* First line = **tension trend** over days:
  * rising â†’ fire drill episode
  * falling â†’ cooling / recovery
  * flat â†’ routine stability
* Per-agent bullets:
  * **â€œmoved from high stress to lowâ€** â†’ `stress_start` â†’ `stress_end`
  * **â€œgradually unwoundâ€** vs â€œheld steadyâ€ vs â€œtightenedâ€ â†’ shape of stress arc
  * Guardrail sentence mirrors `total_guardrail` vs `total_context`.
Use this view for: **High-level â€œpreviously onâ€¦â€** episode summaries (good for dashboards, logs, UI top-level).

### 3.3 Daily Logs (`daily_logs.py` â†’ `--daily-log`)

API:
```
build_daily_log(day_summary, previous_day_summary=None)
```

Outputs a `DailyLog` rendered as:

```
DAILY LOG
----------

Day 1
The floor begins calm and keeps easing off.
[Delta]
- Starts steady but alert â€” always chasing efficiency.
- Leans heavily on protocol.
- Stress eased compared to yesterday.
[Nova]
- ...
[Sprocket]
- ...
General:
- Supervisor stayed mostly quiet.
- Work skewed toward protocol.
- Overall stress eased a notch.
The day ends balanced and steady.
```

**How to interpret:**
This is basically a **shift report:**
* Agent bullets:
  * â€œStress eased compared to yesterdayâ€ â†’ comparison with previous dayâ€™s `avg_stress`.
  * Always uses protocol/context skew lines to reflect `guardrail_count` vs `context_count`.
* General section:
  * **Supervisor** line is a proxy for how often the supervisor acted vs broadcast.
  * **Work skewed toward protocol / context** â†’ city-wide mode distribution.
  * **Overall stress drift** â†’ aggregated stress change.

Use this view for:
**Ops-style analysis** â€“ â€œwhat did the day look like from a shift leadâ€™s POV?â€

### 3.4 Agent Explainer (`explainer.py` â†’ `explain-episode`)

API:
```
build_episode_context(...)
build_agent_focus_context(...)
explain_agent_episode(agent_context)
```

CLI:
```
uv run loopforge-sim explain-episode --steps-per-day 20 --days 3 --agent Delta
```

```
Example:

EPISODE EXPLAINER
==================
Agent: Delta

Delta (optimizer) spent this episode working under a easing factory tension profile.
As an optimizer, they are always watching throughput and deadlines.
Their stress gradually unwound, moving from high to moderate.
They stayed strictly within guardrails, rarely acting on raw context.
They managed to relax as the factory itself eased off.
```

**How to interpret:**
Designed for devs / ops, not players:
* First sentence: agent vs episode-wide tension.
* Second: role flavor from CHARACTERS.
* Third: stress arc: â€œtightenedâ€, â€œgradually unwoundâ€, or â€œheld steadyâ€.
* Fourth: guardrail vs context usage.
* Fifth: alignment between personal and global arcs (â€œrelax as the factory eased offâ€).

Use this when you want to answer:
> â€œIs this robot burning out, coasting, or adapting?â€

### 3.5 LLM Lens (`llm_lens.py` â†’ `lens-agent`)

This is the **future-facing contract** for real LLMs.

API:
```
build_llm_perception_lens_input(day_summary, agent_name) -> LLMPerceptionLensInput
fake_llm_perception_lens(input) -> LLMPerceptionLensOutput

build_llm_episode_lens_input(episode_summary, characters, episode_id="ep-0")
fake_llm_episode_lens(input) -> LLMEpisodeLensOutput
```

CLI:
```
uv run loopforge-sim lens-agent --agent Delta --steps-per-day 20 --day-index 0
```

**Example:**

```
LLM PERCEPTION LENS (input)
-----------------------------
{
  "agent_name": "Delta",
  "role": "optimizer",
  "day_index": 0,
  "perception_mode": "accurate",
  "avg_stress": 0.35,
  "guardrail_count": 90,
  "context_count": 0,
  "tension": 0.36,
  "supervisor_tone_hint": "steady"
}

LLM PERCEPTION LENS (fake output)
----------------------------------
{
  "emotional_read": "under pressure and bound by protocol",
  "risk_assessment": "at risk of burnout",
  "suggested_focus": "increase autonomy where safe",
  "supervisor_comment_prompt": "Maintain pace; check assumptions before committing changes."
}
```

**How to interpret:**
* **Input struct** is the contract weâ€™ll **freeze** before plugging in a real LLM.
* **Fake outputs** are rule-based now (deterministic), but show intended shape:
  * Emotion label
  * Risk label
  * Suggested adjustment lever
  * Suggested supervisor comment

In the future, `fake_llm_*` becomes:
* **Offline LLM calls** producing cached guidance
* Or **online calls** with strict timeouts + fallbacks

But the **input/output dataclasses stay stable.**

## 4. CLI Cheat Sheet

For future architects who just want to see stuff:

```bash
# 1. Run a short sim (no DB)
uv run loopforge-sim --no-db --steps 60

# 2. Numeric + narrative + recap + daily logs
uv run loopforge-sim view-episode \
  --steps-per-day 20 --days 3 \
  --narrative --recap --daily-log

# 3. Agent-focused psychological explainer
uv run loopforge-sim explain-episode \
  --steps-per-day 20 --days 3 --agent Delta

# 4. LLM lens input/output preview
uv run loopforge-sim lens-agent \
  --agent Delta --steps-per-day 20 --day-index 0


```

Recommended workflow:
1. **First pass** â€“ `view-episode` with no flags â†’ sanity check stats.
2. **Second pass** â€“ add `--recap` and `--narrative` â†’ watch the episode like a story.
3. **Third pass** â€“ for any â€œweirdâ€ agent, run `explain-episode` and `lens-agent`.

## 5. How to Read the Numbers as a Story
### 5.1 Tension

* `tension` per day â‰ˆ â€œhow sharp does the factory feel?â€
  * > 0.4 â†’ â€œedgeâ€, â€œsharpâ€, â€œon the brinkâ€
  * 0.15â€“0.4 â†’ â€œsteady with subtle edgeâ€
  * < 0.15 â†’ â€œcalmâ€, â€œquiet humâ€
* Trend:
  * Rising â†’ something is brewing.
  * Falling â†’ recovery / cooldown.
  * Flat high â†’ chronic stress; future LLMs can call this out as â€œsystemic riskâ€.

### 5.2 Stress Bands

Per agent `avg_stress` and `stress_start â†’ stress_end`:
* **Low (< 0.08)** â†’ essentially chill.
* **Mid (0.08â€“0.3)** â†’ engaged, but safe.
* **High (> 0.3)** â†’ tension, potential burnout.

Arcs:
* high â†’ low = â€œunwound / decompressedâ€
* low â†’ high = â€œtightened / under mounting pressureâ€
* similar â†’ â€œheld steadyâ€

### 5.3 Guardrails vs Context

* `guardrail_count` >> `context_count`:
  * â€œLeans heavily on protocol / rulebook.â€
  * Good for safety, bad for adaptation.
* `context_count` significant:
  * Later this will show up as â€œimprovises / acts on instinctâ€.
This is the **main lever** for future LLM supervision:
increase autonomy where safe vs. clamp it down when tension is rising.

## 6. Where Real LLMs Will Plug In

This repo already has **hooks** designed:
1. **Perception Lens (Day-level)**
* Input: `LLMPerceptionLensInput`
* Output: `LLMPerceptionLensOutput`
* Usage: change supervisor messaging, adjust thresholds, or log richer â€œpsychologyâ€ without touching the simulation core.
2. **Episode Lens (Episode-level)**
* Input: `LLMEpisodeLensInput`
* Output: `LLMEpisodeLensOutput`
* Usage: â€œseason summaryâ€, themes, risk flags, guidance for tuning parameters.

## Non-negotiables for future architects
* Keep simulation deterministic-ish; LLMs live in **side channels**, not inside the step loop.
* If you add real LLM calls:
  * Wrap them behind functions with **the same signatures** as `fake_llm_*`.
  * Provide **fallbacks** for test mode (either keep the fakes or use fixtures).
  * Never compute **core metrics** (stress, tension, mode counts) from text: those stay telemetry-driven.

## 7. Extending the Cinematic Debugger

If youâ€™re the next architect on this project:
* Want new characters?
  * â†’ Add them in `CHARACTERS` and watch them show up styled across views.
* Want more narrative flavor?
  * â†’ Extend templates in `narrative_viewer.py` / `daily_logs.py` without changing inputs.
* Want LLM-guided supervision?
  * â†’ Swap `fake_llm_*` with real calls, keep the dataclasses, and log both input and output for debugging.
* Want UI?
  * â†’ These text blocks are already composable:
  * Day narrative = â€œstory paneâ€
  * Daily log = â€œops paneâ€
  * Explainer = â€œfocus paneâ€
  * Lens = â€œLLM suggestion paneâ€

## 8. TL;DR for Future You
* The cinematic debugger is an **observation rig**, not a control system (yet).
* Everything is built on **ActionLogEntry â†’ DaySummary â†’ EpisodeSummary.**
* Narratives, recaps, logs, explainers, and lenses are all **pure, deterministic, read-only layers.**
* We already have LLM-ready contracts; you just need to swap out the fake functions when youâ€™re ready for real model calls.
* If youâ€™re about to push a change that makes the output less interesting to read, stop and ask:
> â€œWould The Producer yell at me for making this more boring?â€

If the answer is yes, walk it back and add more style.